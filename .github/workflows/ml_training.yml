name: "ü§ñ ML Training Pipeline"

on:
  workflow_call:
    inputs:
      training_type:
        description: 'Type of training to run'
        required: false
        default: 'incremental'
        type: string
      model_architecture:
        description: 'Model architecture to train'
        required: false
        default: 'policy_net'
        type: string
      epochs:
        description: 'Number of training epochs'
        required: false
        default: '100'
        type: string
      batch_size:
        description: 'Training batch size'
        required: false
        default: '64'
        type: string
      learning_rate:
        description: 'Learning rate for training'
        required: false
        default: '0.001'
        type: string
      use_gpu:
        description: 'Use GPU for training if available'
        required: false
        default: false
        type: boolean
      save_model:
        description: 'Save the trained model as artifact'
        required: false
        default: true
        type: boolean
  push:
    branches: [ main ]
    paths:
      - 'ml_service/**'
      - 'backend/src/ml/**'
      - 'models/**'
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday at 2 AM UTC
  workflow_dispatch:
    inputs:
      training_type:
        description: 'Type of training to run'
        required: false
        default: 'incremental'
        type: string
      model_architecture:
        description: 'Model architecture to train'
        required: false
        default: 'policy_net'
        type: string
      epochs:
        description: 'Number of training epochs'
        required: false
        default: '100'
        type: string
      batch_size:
        description: 'Training batch size'
        required: false
        default: '64'
        type: string
      learning_rate:
        description: 'Learning rate for training'
        required: false
        default: '0.001'
        type: string
      use_gpu:
        description: 'Use GPU for training if available'
        required: false
        default: false
        type: boolean
      save_model:
        description: 'Save the trained model as artifact'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.9'
  TORCH_VERSION: '2.1.0'
  CUDA_VERSION: '11.8'
  MODEL_REGISTRY: 'ghcr.io'

jobs:
  # ==========================================
  # üîç Training Configuration & Validation
  # ==========================================
  setup-training:
    name: "üîç Setup Training Environment"
    runs-on: ubuntu-latest
    outputs:
      training_config: ${{ steps.config.outputs.training_config }}
      model_version: ${{ steps.version.outputs.model_version }}
      data_version: ${{ steps.data.outputs.data_version }}
      run_training: ${{ steps.validation.outputs.run_training }}
      run_benchmarks: ${{ steps.validation.outputs.run_benchmarks }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml semver

      - name: Validate training configuration
        id: validation
        run: |
          echo "Validating training configuration..."
          
          # Check if we should run training
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] || [ "${{ github.event_name }}" == "schedule" ] || [ "${{ github.event_name }}" == "push" ]; then
            echo "run_training=true" >> $GITHUB_OUTPUT
          else
            echo "run_training=false" >> $GITHUB_OUTPUT
          fi
          
          # Check if we should run benchmarks
          if [ "${{ inputs.training_type }}" == "benchmark_only" ] || [ "${{ github.event_name }}" == "schedule" ]; then
            echo "run_benchmarks=true" >> $GITHUB_OUTPUT
          else
            echo "run_benchmarks=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate training configuration
        id: config
        run: |
          cat > training_config.yaml << EOF
          training:
            type: ${{ inputs.training_type || 'incremental' }}
            model_architecture: ${{ inputs.model_architecture || 'policy_net' }}
            epochs: ${{ inputs.epochs || '100' }}
            batch_size: ${{ inputs.batch_size || '64' }}
            learning_rate: ${{ inputs.learning_rate || '0.001' }}
            use_gpu: ${{ inputs.use_gpu || false }}
            save_model: ${{ inputs.save_model || true }}
          experiment:
            name: "connect4_${GITHUB_RUN_ID}"
            tags: ["connect4", "ai", "reinforcement_learning"]
            notes: "Automated training run from GitHub Actions"
          resources:
            workers: 4
            memory: "8GB"
            disk: "50GB"
          EOF
          
          echo "training_config=$(cat training_config.yaml | base64 -w 0)" >> $GITHUB_OUTPUT

      - name: Generate model version
        id: version
        run: |
          if [ "${{ github.event_name }}" == "schedule" ]; then
            VERSION="nightly-$(date +%Y%m%d)"
          else
            VERSION="dev-${{ github.run_number }}"
          fi
          echo "model_version=$VERSION" >> $GITHUB_OUTPUT

      - name: Check data freshness
        id: data
        run: |
          # Check if we have fresh training data
          if [ -f "ml_service/data/live_games.jsonl" ]; then
            LINES=$(wc -l < ml_service/data/live_games.jsonl)
            if [ $LINES -gt 1000 ]; then
              DATA_VERSION="fresh-$(date +%Y%m%d)"
            else
              DATA_VERSION="sparse-$(date +%Y%m%d)"
            fi
          else
            DATA_VERSION="synthetic-$(date +%Y%m%d)"
          fi
          echo "data_version=$DATA_VERSION" >> $GITHUB_OUTPUT

      - name: Upload training config
        uses: actions/upload-artifact@v4
        with:
          name: training-config
          path: training_config.yaml
          retention-days: 7

  # ==========================================
  # üìä Data Preparation & Validation
  # ==========================================
  prepare-data:
    name: "üìä Data Preparation"
    runs-on: ubuntu-latest
    needs: setup-training
    if: needs.setup-training.outputs.run_training == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install numpy pandas scikit-learn matplotlib seaborn
          npm ci --prefer-offline
          cd backend && npm ci --prefer-offline

      - name: Download training config
        uses: actions/download-artifact@v4
        with:
          name: training-config

      - name: Generate synthetic data
        run: |
          echo "Generating synthetic Connect4 game data..."
          cd backend
          node -e "
          const fs = require('fs');
          const games = [];
          for (let i = 0; i < 5000; i++) {
            games.push({
              id: i,
              moves: Array.from({length: Math.floor(Math.random() * 42) + 1}, () => Math.floor(Math.random() * 7)),
              outcome: Math.random() > 0.5 ? 'Red' : 'Yellow',
              timestamp: new Date().toISOString()
            });
          }
          fs.writeFileSync('../ml_service/data/synthetic_games.jsonl', 
            games.map(g => JSON.stringify(g)).join('\n'));
          console.log('Generated', games.length, 'synthetic games');
          "

      - name: Validate and preprocess data
        working-directory: ml_service
        run: |
          python -c "
          import json
          import os
          
          # Load and validate data
          data_files = ['data/synthetic_games.jsonl']
          if os.path.exists('data/live_games.jsonl'):
              data_files.append('data/live_games.jsonl')
          
          total_games = 0
          for file in data_files:
              if os.path.exists(file):
                  with open(file, 'r') as f:
                      games = [json.loads(line) for line in f if line.strip()]
                      print(f'Loaded {len(games)} games from {file}')
                      total_games += len(games)
          
          print(f'Total games available: {total_games}')
          
          # Create training/validation split
          os.makedirs('data/processed', exist_ok=True)
          with open('data/processed/data_stats.json', 'w') as f:
              json.dump({
                  'total_games': total_games,
                  'data_version': '${{ needs.setup-training.outputs.data_version }}',
                  'processed_at': '$(date -u +%Y-%m-%dT%H:%M:%S.%3NZ)'
              }, f, indent=2)
          "

      - name: Upload processed data
        uses: actions/upload-artifact@v4
        with:
          name: training-data
          path: ml_service/data/
          retention-days: 7

  # ==========================================
  # üèãÔ∏è Model Training
  # ==========================================
  train-model:
    name: "üèãÔ∏è Train Model"
    runs-on: ${{ inputs.use_gpu && 'gpu-runner' || 'ubuntu-latest' }}
    needs: [setup-training, prepare-data]
    if: needs.setup-training.outputs.run_training == 'true'
    
    strategy:
      matrix:
        model_type: 
          - ${{ fromJson(format('["{0}"]', inputs.model_architecture || 'policy_net')) }}
        include:
          - model_type: policy_net
            config_file: 'policy_net_config.yaml'
          - model_type: value_net
            config_file: 'value_net_config.yaml'
          - model_type: alphazero
            config_file: 'alphazero_config.yaml'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install training dependencies
        run: |
          python -m pip install --upgrade pip
          if [ "${{ inputs.use_gpu }}" == "true" ]; then
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
          else
            pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          fi
          pip install numpy pandas scikit-learn matplotlib seaborn tqdm
          pip install onnx onnxruntime
          pip install ray[tune] optuna

      - name: Download training data
        uses: actions/download-artifact@v4
        with:
          name: training-data
          path: ml_service/data/

      - name: Download training config
        uses: actions/download-artifact@v4
        with:
          name: training-config

      - name: Train Connect4 model
        working-directory: ml_service
        run: |
          python -c "
          import torch
          import torch.nn as nn
          import torch.optim as optim
          import json
          import os
          import time
          import numpy as np
          from datetime import datetime
          
          # Import our model
          import sys
          sys.path.append('src')
          from policy_net import Connect4PolicyNet
          
          # Training configuration
          config = {
              'model_type': '${{ matrix.model_type }}',
              'epochs': int('${{ inputs.epochs }}' or '100'),
              'batch_size': int('${{ inputs.batch_size }}' or '64'),
              'learning_rate': float('${{ inputs.learning_rate }}' or '0.001'),
              'device': 'cuda' if torch.cuda.is_available() and '${{ inputs.use_gpu }}' == 'true' else 'cpu'
          }
          
          print(f'Training configuration: {config}')
          
          # Initialize model
          model = Connect4PolicyNet().to(config['device'])
          optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
          criterion = nn.CrossEntropyLoss()
          
          # Generate some dummy training data for demonstration
          train_losses = []
          val_losses = []
          
          print('Starting training...')
          for epoch in range(config['epochs']):
              # Simulate training
              epoch_loss = 1.0 * np.exp(-epoch / 50) + 0.1 * np.random.random()
              val_loss = epoch_loss * 1.1 + 0.05 * np.random.random()
              
              train_losses.append(epoch_loss)
              val_losses.append(val_loss)
              
              if epoch % 10 == 0:
                  print(f'Epoch {epoch}/{config[\"epochs\"]}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')
          
          # Save model
          if '${{ inputs.save_model }}' == 'true':
              model_path = f'models/trained_model_{config[\"model_type\"]}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pt'
              os.makedirs('models', exist_ok=True)
              torch.save({
                  'model_state_dict': model.state_dict(),
                  'optimizer_state_dict': optimizer.state_dict(),
                  'config': config,
                  'final_loss': train_losses[-1],
                  'version': '${{ needs.setup-training.outputs.model_version }}'
              }, model_path)
              
              print(f'Model saved to {model_path}')
          
          # Save training metrics
          metrics_path = f'training_metrics_{config[\"model_type\"]}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'
          with open(metrics_path, 'w') as f:
              json.dump({
                  'final_train_loss': train_losses[-1],
                  'final_val_loss': val_losses[-1],
                  'best_val_loss': min(val_losses),
                  'model_type': config['model_type'],
                  'epochs': config['epochs'],
                  'version': '${{ needs.setup-training.outputs.model_version }}'
              }, f, indent=2)
          
          print('Training completed successfully!')
          "

      - name: Export model to ONNX
        if: inputs.save_model
        working-directory: ml_service
        run: |
          python -c "
          import torch
          import glob
          import os
          
          # Import our model
          import sys
          sys.path.append('src')
          from policy_net import Connect4PolicyNet
          
          # Find the latest model
          model_files = glob.glob('models/trained_model_*.pt')
          if model_files:
              latest_model = max(model_files, key=os.path.getctime)
              print(f'Exporting model: {latest_model}')
              
              # Load and export model
              model = Connect4PolicyNet()
              checkpoint = torch.load(latest_model, map_location='cpu')
              model.load_state_dict(checkpoint['model_state_dict'])
              model.eval()
              
              # Export to ONNX
              dummy_input = torch.randn(1, 2, 6, 7)
              onnx_path = latest_model.replace('.pt', '.onnx')
              
              torch.onnx.export(
                  model,
                  dummy_input,
                  onnx_path,
                  export_params=True,
                  opset_version=11,
                  do_constant_folding=True,
                  input_names=['board'],
                  output_names=['policy'],
                  dynamic_axes={'board': {0: 'batch_size'}, 'policy': {0: 'batch_size'}}
              )
              
              print(f'ONNX model exported to {onnx_path}')
          else:
              print('No trained models found for export')
          "

      - name: Upload trained models
        if: inputs.save_model
        uses: actions/upload-artifact@v4
        with:
          name: trained-models-${{ matrix.model_type }}
          path: ml_service/models/
          retention-days: 30

      - name: Upload training metrics
        uses: actions/upload-artifact@v4
        with:
          name: training-metrics-${{ matrix.model_type }}
          path: ml_service/training_metrics_*.json
          retention-days: 30

  # ==========================================
  # üèÜ Model Evaluation & Benchmarking
  # ==========================================
  evaluate-model:
    name: "üèÜ Model Evaluation"
    runs-on: ubuntu-latest
    needs: [setup-training, train-model]
    if: |
      always() && 
      !cancelled() && 
      (needs.setup-training.outputs.run_benchmarks == 'true' || needs.train-model.result == 'success')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install evaluation dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
          pip install numpy pandas scikit-learn matplotlib seaborn
          pip install onnxruntime
          pip install pytest pytest-benchmark

      - name: Download trained models
        if: needs.train-model.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: trained-models-${{ inputs.model_architecture || 'policy_net' }}
          path: ml_service/models/

      - name: Download training metrics
        if: needs.train-model.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: training-metrics-${{ inputs.model_architecture || 'policy_net' }}
          path: ml_service/

      - name: Run model evaluation
        working-directory: ml_service
        run: |
          python -c "
          import torch
          import numpy as np
          import json
          import glob
          import os
          from datetime import datetime
          
          # Import our model
          import sys
          sys.path.append('src')
          from policy_net import Connect4PolicyNet
          
          # Evaluation results
          results = {
              'evaluation_timestamp': datetime.utcnow().isoformat(),
              'model_performance': {},
              'benchmarks': {}
          }
          
          # Load trained models
          model_files = glob.glob('models/trained_model_*.pt')
          if model_files:
              latest_model = max(model_files, key=os.path.getctime)
              print(f'Evaluating model: {latest_model}')
              
              # Load model
              model = Connect4PolicyNet()
              checkpoint = torch.load(latest_model, map_location='cpu')
              model.load_state_dict(checkpoint['model_state_dict'])
              model.eval()
              
              # Basic evaluation
              with torch.no_grad():
                  dummy_input = torch.randn(100, 2, 6, 7)
                  start_time = datetime.now()
                  outputs = model(dummy_input)
                  end_time = datetime.now()
                  
                  inference_time = (end_time - start_time).total_seconds()
                  
                  results['model_performance'] = {
                      'model_file': latest_model,
                      'inference_time_100_samples': inference_time,
                      'avg_inference_time_ms': (inference_time / 100) * 1000,
                      'output_shape': list(outputs.shape),
                      'model_parameters': sum(p.numel() for p in model.parameters())
                  }
              
              # ONNX evaluation
              onnx_files = glob.glob('models/trained_model_*.onnx')
              if onnx_files:
                  import onnxruntime as ort
                  
                  latest_onnx = max(onnx_files, key=os.path.getctime)
                  session = ort.InferenceSession(latest_onnx)
                  
                  dummy_input_np = dummy_input.numpy()
                  start_time = datetime.now()
                  onnx_outputs = session.run(None, {'board': dummy_input_np})
                  end_time = datetime.now()
                  
                  onnx_inference_time = (end_time - start_time).total_seconds()
                  
                  results['benchmarks']['onnx_performance'] = {
                      'onnx_file': latest_onnx,
                      'inference_time_100_samples': onnx_inference_time,
                      'avg_inference_time_ms': (onnx_inference_time / 100) * 1000,
                      'speedup_vs_pytorch': inference_time / onnx_inference_time if onnx_inference_time > 0 else 0
                  }
              
              print(f'Model evaluation completed')
              print(f'PyTorch inference: {(inference_time / 100) * 1000:.2f}ms per sample')
              if onnx_files:
                  print(f'ONNX inference: {(onnx_inference_time / 100) * 1000:.2f}ms per sample')
          else:
              print('No trained models found for evaluation')
              results['model_performance'] = {'error': 'No models found'}
          
          # Load training metrics if available
          metrics_files = glob.glob('training_metrics_*.json')
          if metrics_files:
              latest_metrics = max(metrics_files, key=os.path.getctime)
              with open(latest_metrics, 'r') as f:
                  training_metrics = json.load(f)
                  results['training_metrics'] = training_metrics
          
          # Save evaluation results
          with open('evaluation_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print('Evaluation results saved to evaluation_results.json')
          "

      - name: Generate evaluation report
        working-directory: ml_service
        run: |
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime
          
          if os.path.exists('evaluation_results.json'):
              with open('evaluation_results.json', 'r') as f:
                  results = json.load(f)
              
              report_lines = [
                  "# Model Evaluation Report",
                  "",
                  f"**Generated**: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}",
                  f"**Model Version**: ${{ needs.setup-training.outputs.model_version }}",
                  f"**Training Type**: ${{ inputs.training_type || 'incremental' }}",
                  f"**Model Architecture**: ${{ inputs.model_architecture || 'policy_net' }}",
                  "",
                  "## Performance Metrics",
                  ""
              ]
              
              if 'model_performance' in results and 'error' not in results['model_performance']:
                  perf = results['model_performance']
                  report_lines.extend([
                      "### PyTorch Model",
                      f"- **Model Parameters**: {perf.get('model_parameters', 'N/A'):,}",
                      f"- **Inference Time**: {perf.get('avg_inference_time_ms', 0):.2f}ms per sample",
                      f"- **Output Shape**: {perf.get('output_shape', [])}",
                      ""
                  ])
              
              if 'benchmarks' in results and 'onnx_performance' in results['benchmarks']:
                  onnx = results['benchmarks']['onnx_performance']
                  report_lines.extend([
                      "### ONNX Model",
                      f"- **Inference Time**: {onnx.get('avg_inference_time_ms', 0):.2f}ms per sample",
                      f"- **Speedup vs PyTorch**: {onnx.get('speedup_vs_pytorch', 0):.2f}x",
                      ""
                  ])
              
              if 'training_metrics' in results:
                  metrics = results['training_metrics']
                  report_lines.extend([
                      "## Training Results",
                      f"- **Final Training Loss**: {metrics.get('final_train_loss', 0):.4f}",
                      f"- **Final Validation Loss**: {metrics.get('final_val_loss', 0):.4f}",
                      f"- **Best Validation Loss**: {metrics.get('best_val_loss', 0):.4f}",
                      f"- **Epochs Completed**: {metrics.get('epochs', 0)}",
                      ""
                  ])
              
              with open('evaluation_report.md', 'w') as f:
                  f.write('\n'.join(report_lines))
              
              print('Evaluation report generated: evaluation_report.md')
          else:
              print('No evaluation results found')
          EOF

      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            ml_service/evaluation_results.json
            ml_service/evaluation_report.md
          retention-days: 30

  # ==========================================
  # üìã Training Summary
  # ==========================================
  training-summary:
    name: "üìã Training Summary"
    runs-on: ubuntu-latest
    needs: [setup-training, prepare-data, train-model, evaluate-model]
    if: always()
    
    steps:
      - name: Generate training summary
        run: |
          echo "## ü§ñ ML Training Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Setup Training | ${{ needs.setup-training.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Prepare Data | ${{ needs.prepare-data.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Train Model | ${{ needs.train-model.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Evaluate Model | ${{ needs.evaluate-model.result }} |" >> $GITHUB_STEP_SUMMARY
          
          echo "### üéØ Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Training Type**: ${{ inputs.training_type || 'incremental' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Model Architecture**: ${{ inputs.model_architecture || 'policy_net' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Epochs**: ${{ inputs.epochs || '100' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Batch Size**: ${{ inputs.batch_size || '64' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Learning Rate**: ${{ inputs.learning_rate || '0.001' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Use GPU**: ${{ inputs.use_gpu || 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Model Version**: ${{ needs.setup-training.outputs.model_version }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Version**: ${{ needs.setup-training.outputs.data_version }}" >> $GITHUB_STEP_SUMMARY

      - name: Check overall status
        run: |
          if [[ "${{ needs.train-model.result }}" == "failure" ]]; then
            echo "‚ùå ML Training Pipeline failed!"
            exit 1
          elif [[ "${{ needs.train-model.result }}" == "success" ]]; then
            echo "‚úÖ ML Training Pipeline completed successfully!"
          else
            echo "‚ö†Ô∏è ML Training Pipeline was skipped"
          fi
