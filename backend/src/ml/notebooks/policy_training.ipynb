{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Import your policy network\n",
    "ML_ROOT = Path(__file__).resolve().parent.parent\n",
    "sys.path.append(str(ML_ROOT / 'src'))\n",
    "from policy_net import Connect4PolicyNet  # type: ignore\n",
    "\n",
    "\n",
    "def find_file(filename: str) -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    for ancestor in [cwd] + list(cwd.parents):\n",
    "        candidate = ancestor / 'backend' / 'src' / 'ml' / 'data' / filename\n",
    "        if candidate.is_file():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not locate {filename} under any ancestor directory\")\n",
    "\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def convert_feature(feat_list):\n",
    "    # feat_list: list of 42 ints (0=Empty,1=Red,2=Yellow)\n",
    "    arr = np.array(feat_list, dtype=np.int64).reshape(6, 7)\n",
    "    red = (arr == 1).astype(np.float32)\n",
    "    yellow = (arr == 2).astype(np.float32)\n",
    "    return torch.from_numpy(np.stack([red, yellow], axis=0))  # shape [2,6,7]\n",
    "\n",
    "\n",
    "def prepare_dataset(json_path: Path):\n",
    "    examples = load_json(json_path)\n",
    "    data_list, label_list = [], []\n",
    "    for ex in examples:\n",
    "        feat = ex.get('features')\n",
    "        label = ex.get('label')\n",
    "        if feat is None or label is None:\n",
    "            continue\n",
    "        data_list.append(convert_feature(feat))\n",
    "        label_list.append(int(label))\n",
    "    if not data_list:\n",
    "        raise RuntimeError(f\"No valid examples found in {json_path}\")\n",
    "    data = torch.stack(data_list)      # [N,2,6,7]\n",
    "    labels = torch.tensor(label_list, dtype=torch.long)\n",
    "    return TensorDataset(data, labels)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total * 100\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Supervised policy training for Connect4\")\n",
    "    parser.add_argument('--train-json', type=Path,\n",
    "                        help=\"Path to train.json file\")\n",
    "    parser.add_argument('--test-data', type=Path,\n",
    "                        help=\"Path to test_data.pt file for evaluation\")\n",
    "    parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help=\"Number of training epochs\")\n",
    "    parser.add_argument('--batch-size', type=int, default=64,\n",
    "                        help=\"Batch size\")\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help=\"Learning rate\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Locate train.json\n",
    "    if args.train_json:\n",
    "        train_path = args.train_json\n",
    "    else:\n",
    "        try:\n",
    "            train_path = find_file('train.json')\n",
    "        except FileNotFoundError as e:\n",
    "            sys.stderr.write(str(e) + \"\\n\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    # Prepare dataset & loader\n",
    "    train_ds = prepare_dataset(train_path)\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    # Optionally prepare test loader\n",
    "    test_loader = None\n",
    "    if args.test_data:\n",
    "        chk = torch.load(args.test_data, map_location='cpu')\n",
    "        test_ds = TensorDataset(chk['data'], chk['labels'])\n",
    "        test_loader = DataLoader(test_ds, batch_size=args.batch_size)\n",
    "\n",
    "    # Device and model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Connect4PolicyNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Paths for export\n",
    "    models_dir = ML_ROOT / 'models'\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = models_dir / 'best_policy_net.pt'\n",
    "    ts_path = models_dir / 'policy_net_ts.pt'\n",
    "    onnx_path = models_dir / 'policy_net.onnx'\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "        avg_loss = running_loss / len(train_ds)\n",
    "\n",
    "        # Evaluate on test set if available\n",
    "        if test_loader:\n",
    "            acc = evaluate(model, test_loader, device)\n",
    "            print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, TestAcc={acc:.2f}%\")\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: Loss={avg_loss:.4f}\")\n",
    "            # Save checkpoint every epoch\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "    print(f\"Training complete. Best test accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "    # Export TorchScript\n",
    "    model_cpu = Connect4PolicyNet()\n",
    "    model_cpu.load_state_dict(torch.load(ckpt_path, map_location='cpu'))\n",
    "    model_cpu.eval()\n",
    "    scripted = torch.jit.script(model_cpu)\n",
    "    scripted.save(ts_path)\n",
    "    print(f\"Saved TorchScript model to {ts_path}\")\n",
    "\n",
    "    # Export ONNX\n",
    "    dummy = torch.randn(1, 2, 6, 7)\n",
    "    torch.onnx.export(\n",
    "        model_cpu, dummy, onnx_path,\n",
    "        input_names=['input'], output_names=['logits'],\n",
    "        dynamic_axes={'input':{0:'batch'}, 'logits':{0:'batch'}}\n",
    "    )\n",
    "    print(f\"Saved ONNX model to {onnx_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
